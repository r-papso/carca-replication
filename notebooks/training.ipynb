{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next-step: one-hot without context and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from carca.data import CARCADataset, load_attrs, load_ctx, load_profiles, set_datapath\n",
    "from carca.model import CARCA\n",
    "from carca.train import train, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_datapath(\"../../data/embedding_experiment/video_games/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = load_attrs(\"video_games_sbert_5core.dat\")\n",
    "ctx = load_ctx(\"video_games_ctx_5core.dat\")\n",
    "user_ids, item_ids, profiles = load_profiles(\"video_games_sorted_5core.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"embedding_experiment_onehot\"\n",
    "n_items = attrs.shape[0]\n",
    "n_ctx = next(iter(ctx.values())).shape[0]\n",
    "n_attrs = attrs.shape[1]\n",
    "embeddings = \"attr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "learning_rate =  0.0001\n",
    "seq_len = 50\n",
    "n_blocks = 3\n",
    "n_heads = 3\n",
    "dropout_rate = 0.5\n",
    "l2_reg = 0.0\n",
    "d_dim = 90\n",
    "g_dim = 450\n",
    "residual_sa = True\n",
    "residual_ca = True\n",
    "epochs = 800\n",
    "batch_size = 128\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CARCADataset(\n",
    "    user_ids=user_ids,\n",
    "    item_ids=item_ids,\n",
    "    profiles=profiles,\n",
    "    attrs=attrs,\n",
    "    ctx=ctx,\n",
    "    profile_seq_len=seq_len,\n",
    "    target_seq_len=100,\n",
    "    mode=\"train\"\n",
    ")\n",
    "val_data = CARCADataset(\n",
    "    user_ids=user_ids,\n",
    "    item_ids=item_ids,\n",
    "    profiles=profiles,\n",
    "    attrs=attrs,\n",
    "    ctx=ctx,\n",
    "    profile_seq_len=seq_len,\n",
    "    target_seq_len=100,\n",
    "    mode=\"val\"\n",
    ")\n",
    "test_data = CARCADataset(\n",
    "    user_ids=user_ids,\n",
    "    item_ids=item_ids,\n",
    "    profiles=profiles,\n",
    "    attrs=attrs,\n",
    "    ctx=ctx,\n",
    "    profile_seq_len=seq_len,\n",
    "    target_seq_len=100,\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "val_idx = random.sample(range(len(val_data)), 10_000) if len(val_data) > 10_000 else range(len(val_data))\n",
    "val_sub = Subset(val_data, val_idx)\n",
    "test_idx = random.sample(range(len(test_data)), 10_000) if len(test_data) > 10_000 else range(len(test_data))\n",
    "test_sub = Subset(test_data, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_sub, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_sub, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CARCA(\n",
    "    n_items=n_items,\n",
    "    d=d_dim,\n",
    "    g=g_dim,\n",
    "    n_ctx=n_ctx,\n",
    "    n_attrs=n_attrs,\n",
    "    H=n_heads,\n",
    "    p=dropout_rate,\n",
    "    B=n_blocks,\n",
    "    res_sa=residual_sa,\n",
    "    res_ca=residual_ca,\n",
    "    embeddings=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\"\n",
    "print(f\"Using {device} device\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:48:55 - Epoch 001: Loss = 0.5977\n",
      "20:49:02 - Epoch 001: Loss = 0.5378 HR = 0.5282, NDCG = 0.3168\n",
      "20:49:34 - Epoch 002: Loss = 0.4783\n",
      "20:49:40 - Epoch 002: Loss = 0.5474 HR = 0.5738, NDCG = 0.3376\n",
      "20:50:10 - Epoch 003: Loss = 0.4558\n",
      "20:50:16 - Epoch 003: Loss = 0.5055 HR = 0.5602, NDCG = 0.3325\n",
      "20:50:44 - Epoch 004: Loss = 0.4392\n",
      "20:50:50 - Epoch 004: Loss = 0.4968 HR = 0.5751, NDCG = 0.3417\n",
      "20:51:18 - Epoch 005: Loss = 0.4279\n",
      "20:51:24 - Epoch 005: Loss = 0.4941 HR = 0.5826, NDCG = 0.3461\n",
      "20:51:52 - Epoch 006: Loss = 0.4191\n",
      "20:51:58 - Epoch 006: Loss = 0.4896 HR = 0.5849, NDCG = 0.3511\n",
      "20:52:26 - Epoch 007: Loss = 0.4131\n",
      "20:52:31 - Epoch 007: Loss = 0.4929 HR = 0.5964, NDCG = 0.3585\n",
      "20:53:00 - Epoch 008: Loss = 0.4077\n",
      "20:53:05 - Epoch 008: Loss = 0.4870 HR = 0.5930, NDCG = 0.3536\n",
      "20:53:33 - Epoch 009: Loss = 0.4029\n",
      "20:53:39 - Epoch 009: Loss = 0.4826 HR = 0.6040, NDCG = 0.3634\n",
      "20:54:07 - Epoch 010: Loss = 0.3955\n",
      "20:54:13 - Epoch 010: Loss = 0.4643 HR = 0.6096, NDCG = 0.3671\n",
      "20:54:41 - Epoch 011: Loss = 0.3860\n",
      "20:54:46 - Epoch 011: Loss = 0.4627 HR = 0.6188, NDCG = 0.3793\n",
      "20:55:15 - Epoch 012: Loss = 0.3751\n",
      "20:55:20 - Epoch 012: Loss = 0.4308 HR = 0.6308, NDCG = 0.3905\n",
      "20:55:49 - Epoch 013: Loss = 0.3651\n",
      "20:55:54 - Epoch 013: Loss = 0.4268 HR = 0.6431, NDCG = 0.4058\n",
      "20:56:23 - Epoch 014: Loss = 0.3555\n",
      "20:56:28 - Epoch 014: Loss = 0.4039 HR = 0.6540, NDCG = 0.4129\n",
      "20:56:57 - Epoch 015: Loss = 0.3488\n",
      "20:57:02 - Epoch 015: Loss = 0.3914 HR = 0.6635, NDCG = 0.4177\n",
      "20:57:31 - Epoch 016: Loss = 0.3429\n",
      "20:57:36 - Epoch 016: Loss = 0.3812 HR = 0.6632, NDCG = 0.4187\n",
      "20:58:04 - Epoch 017: Loss = 0.3382\n",
      "20:58:10 - Epoch 017: Loss = 0.3915 HR = 0.6704, NDCG = 0.4244\n",
      "20:58:38 - Epoch 018: Loss = 0.3343\n",
      "20:58:43 - Epoch 018: Loss = 0.3743 HR = 0.6675, NDCG = 0.4256\n",
      "20:59:12 - Epoch 019: Loss = 0.3294\n",
      "20:59:17 - Epoch 019: Loss = 0.3859 HR = 0.6814, NDCG = 0.4360\n",
      "20:59:45 - Epoch 020: Loss = 0.3245\n",
      "20:59:51 - Epoch 020: Loss = 0.3754 HR = 0.6803, NDCG = 0.4358\n",
      "21:00:20 - Epoch 021: Loss = 0.3205\n",
      "21:00:25 - Epoch 021: Loss = 0.3640 HR = 0.6834, NDCG = 0.4390\n",
      "21:00:54 - Epoch 022: Loss = 0.3169\n",
      "21:00:59 - Epoch 022: Loss = 0.3617 HR = 0.6833, NDCG = 0.4393\n",
      "21:01:28 - Epoch 023: Loss = 0.3139\n",
      "21:01:33 - Epoch 023: Loss = 0.3565 HR = 0.6884, NDCG = 0.4438\n",
      "21:02:02 - Epoch 024: Loss = 0.3077\n",
      "21:02:07 - Epoch 024: Loss = 0.3528 HR = 0.6944, NDCG = 0.4532\n",
      "21:02:36 - Epoch 025: Loss = 0.3030\n",
      "21:02:41 - Epoch 025: Loss = 0.3523 HR = 0.6971, NDCG = 0.4497\n",
      "21:03:10 - Epoch 026: Loss = 0.3010\n",
      "21:03:15 - Epoch 026: Loss = 0.3435 HR = 0.6955, NDCG = 0.4513\n",
      "21:03:44 - Epoch 027: Loss = 0.2979\n",
      "21:03:49 - Epoch 027: Loss = 0.3384 HR = 0.7059, NDCG = 0.4587\n",
      "21:04:18 - Epoch 028: Loss = 0.2945\n",
      "21:04:23 - Epoch 028: Loss = 0.3205 HR = 0.7056, NDCG = 0.4573\n",
      "21:04:52 - Epoch 029: Loss = 0.2915\n",
      "21:04:57 - Epoch 029: Loss = 0.3162 HR = 0.7042, NDCG = 0.4613\n",
      "21:05:26 - Epoch 030: Loss = 0.2888\n",
      "21:05:31 - Epoch 030: Loss = 0.3253 HR = 0.7022, NDCG = 0.4608\n",
      "21:06:00 - Epoch 031: Loss = 0.2841\n",
      "21:06:05 - Epoch 031: Loss = 0.3164 HR = 0.7099, NDCG = 0.4685\n",
      "21:06:34 - Epoch 032: Loss = 0.2814\n",
      "21:06:39 - Epoch 032: Loss = 0.3191 HR = 0.7057, NDCG = 0.4639\n",
      "21:07:07 - Epoch 033: Loss = 0.2776\n",
      "21:07:13 - Epoch 033: Loss = 0.3016 HR = 0.7104, NDCG = 0.4692\n",
      "21:07:41 - Epoch 034: Loss = 0.2754\n",
      "21:07:47 - Epoch 034: Loss = 0.3029 HR = 0.7114, NDCG = 0.4719\n",
      "21:08:15 - Epoch 035: Loss = 0.2725\n",
      "21:08:21 - Epoch 035: Loss = 0.3107 HR = 0.7166, NDCG = 0.4766\n",
      "21:08:49 - Epoch 036: Loss = 0.2695\n",
      "21:08:55 - Epoch 036: Loss = 0.2945 HR = 0.7163, NDCG = 0.4775\n",
      "21:09:23 - Epoch 037: Loss = 0.2655\n",
      "21:09:29 - Epoch 037: Loss = 0.3029 HR = 0.7180, NDCG = 0.4832\n",
      "21:09:57 - Epoch 038: Loss = 0.2614\n",
      "21:10:02 - Epoch 038: Loss = 0.2818 HR = 0.7253, NDCG = 0.4900\n",
      "21:10:30 - Epoch 039: Loss = 0.2585\n",
      "21:10:36 - Epoch 039: Loss = 0.2815 HR = 0.7292, NDCG = 0.4907\n",
      "21:11:04 - Epoch 040: Loss = 0.2562\n",
      "21:11:10 - Epoch 040: Loss = 0.2840 HR = 0.7336, NDCG = 0.4929\n",
      "21:11:38 - Epoch 041: Loss = 0.2535\n",
      "21:11:43 - Epoch 041: Loss = 0.2749 HR = 0.7278, NDCG = 0.4904\n",
      "21:12:11 - Epoch 042: Loss = 0.2496\n",
      "21:12:17 - Epoch 042: Loss = 0.2702 HR = 0.7336, NDCG = 0.4985\n",
      "21:12:45 - Epoch 043: Loss = 0.2484\n",
      "21:12:51 - Epoch 043: Loss = 0.2696 HR = 0.7368, NDCG = 0.5006\n",
      "21:13:19 - Epoch 044: Loss = 0.2453\n",
      "21:13:25 - Epoch 044: Loss = 0.2731 HR = 0.7367, NDCG = 0.5007\n",
      "21:13:53 - Epoch 045: Loss = 0.2431\n",
      "21:13:58 - Epoch 045: Loss = 0.2655 HR = 0.7427, NDCG = 0.5068\n",
      "21:14:26 - Epoch 046: Loss = 0.2400\n",
      "21:14:32 - Epoch 046: Loss = 0.2570 HR = 0.7406, NDCG = 0.5060\n",
      "21:15:00 - Epoch 047: Loss = 0.2365\n",
      "21:15:06 - Epoch 047: Loss = 0.2591 HR = 0.7402, NDCG = 0.5095\n",
      "21:15:34 - Epoch 048: Loss = 0.2351\n",
      "21:15:39 - Epoch 048: Loss = 0.2461 HR = 0.7376, NDCG = 0.5019\n",
      "21:16:07 - Epoch 049: Loss = 0.2326\n",
      "21:16:13 - Epoch 049: Loss = 0.2422 HR = 0.7448, NDCG = 0.5125\n",
      "21:16:41 - Epoch 050: Loss = 0.2300\n",
      "21:16:46 - Epoch 050: Loss = 0.2391 HR = 0.7447, NDCG = 0.5123\n",
      "21:17:15 - Epoch 051: Loss = 0.2273\n",
      "21:17:20 - Epoch 051: Loss = 0.2497 HR = 0.7469, NDCG = 0.5143\n",
      "21:17:48 - Epoch 052: Loss = 0.2244\n",
      "21:17:54 - Epoch 052: Loss = 0.2379 HR = 0.7473, NDCG = 0.5161\n",
      "21:18:22 - Epoch 053: Loss = 0.2218\n",
      "21:18:27 - Epoch 053: Loss = 0.2176 HR = 0.7461, NDCG = 0.5135\n",
      "21:18:55 - Epoch 054: Loss = 0.2203\n",
      "21:19:01 - Epoch 054: Loss = 0.2290 HR = 0.7508, NDCG = 0.5191\n",
      "21:19:29 - Epoch 055: Loss = 0.2184\n",
      "21:19:35 - Epoch 055: Loss = 0.2254 HR = 0.7527, NDCG = 0.5232\n",
      "21:20:03 - Epoch 056: Loss = 0.2171\n",
      "21:20:08 - Epoch 056: Loss = 0.2153 HR = 0.7515, NDCG = 0.5248\n",
      "21:20:36 - Epoch 057: Loss = 0.2142\n",
      "21:20:42 - Epoch 057: Loss = 0.2086 HR = 0.7511, NDCG = 0.5208\n",
      "21:21:10 - Epoch 058: Loss = 0.2129\n",
      "21:21:15 - Epoch 058: Loss = 0.2027 HR = 0.7509, NDCG = 0.5228\n",
      "21:21:44 - Epoch 059: Loss = 0.2108\n",
      "21:21:49 - Epoch 059: Loss = 0.2124 HR = 0.7501, NDCG = 0.5160\n",
      "21:22:17 - Epoch 060: Loss = 0.2085\n",
      "21:22:23 - Epoch 060: Loss = 0.1935 HR = 0.7483, NDCG = 0.5194\n",
      "21:22:51 - Epoch 061: Loss = 0.2068\n",
      "21:22:56 - Epoch 061: Loss = 0.2048 HR = 0.7567, NDCG = 0.5252\n",
      "21:23:25 - Epoch 062: Loss = 0.2051\n",
      "21:23:30 - Epoch 062: Loss = 0.2107 HR = 0.7569, NDCG = 0.5293\n",
      "21:23:58 - Epoch 063: Loss = 0.2054\n",
      "21:24:04 - Epoch 063: Loss = 0.2001 HR = 0.7544, NDCG = 0.5274\n",
      "21:24:32 - Epoch 064: Loss = 0.2022\n",
      "21:24:37 - Epoch 064: Loss = 0.2010 HR = 0.7564, NDCG = 0.5279\n",
      "21:25:06 - Epoch 065: Loss = 0.2015\n",
      "21:25:11 - Epoch 065: Loss = 0.2011 HR = 0.7566, NDCG = 0.5265\n",
      "21:25:39 - Epoch 066: Loss = 0.1991\n",
      "21:25:45 - Epoch 066: Loss = 0.1979 HR = 0.7559, NDCG = 0.5246\n",
      "21:26:13 - Epoch 067: Loss = 0.1974\n",
      "21:26:18 - Epoch 067: Loss = 0.2006 HR = 0.7570, NDCG = 0.5288\n",
      "21:26:46 - Epoch 068: Loss = 0.1963\n",
      "21:26:52 - Epoch 068: Loss = 0.1960 HR = 0.7553, NDCG = 0.5283\n",
      "21:27:20 - Epoch 069: Loss = 0.1953\n",
      "21:27:26 - Epoch 069: Loss = 0.1866 HR = 0.7592, NDCG = 0.5302\n",
      "21:27:54 - Epoch 070: Loss = 0.1939\n",
      "21:28:00 - Epoch 070: Loss = 0.1972 HR = 0.7595, NDCG = 0.5348\n",
      "21:28:28 - Epoch 071: Loss = 0.1927\n",
      "21:28:34 - Epoch 071: Loss = 0.1938 HR = 0.7562, NDCG = 0.5301\n",
      "21:29:02 - Epoch 072: Loss = 0.1904\n",
      "21:29:07 - Epoch 072: Loss = 0.1939 HR = 0.7552, NDCG = 0.5325\n",
      "21:29:35 - Epoch 073: Loss = 0.1894\n",
      "21:29:41 - Epoch 073: Loss = 0.1868 HR = 0.7572, NDCG = 0.5307\n",
      "21:30:09 - Epoch 074: Loss = 0.1886\n",
      "21:30:14 - Epoch 074: Loss = 0.1928 HR = 0.7587, NDCG = 0.5320\n",
      "21:30:42 - Epoch 075: Loss = 0.1867\n",
      "21:30:48 - Epoch 075: Loss = 0.1940 HR = 0.7584, NDCG = 0.5321\n",
      "21:31:16 - Epoch 076: Loss = 0.1861\n",
      "21:31:22 - Epoch 076: Loss = 0.1850 HR = 0.7562, NDCG = 0.5314\n",
      "21:31:50 - Epoch 077: Loss = 0.1848\n",
      "21:31:55 - Epoch 077: Loss = 0.1847 HR = 0.7559, NDCG = 0.5309\n",
      "21:32:23 - Epoch 078: Loss = 0.1836\n",
      "21:32:29 - Epoch 078: Loss = 0.1772 HR = 0.7547, NDCG = 0.5307\n",
      "21:32:57 - Epoch 079: Loss = 0.1839\n",
      "21:33:02 - Epoch 079: Loss = 0.1930 HR = 0.7594, NDCG = 0.5320\n",
      "21:33:31 - Epoch 080: Loss = 0.1824\n",
      "21:33:36 - Epoch 080: Loss = 0.1841 HR = 0.7601, NDCG = 0.5366\n",
      "21:34:04 - Epoch 081: Loss = 0.1815\n",
      "21:34:10 - Epoch 081: Loss = 0.1856 HR = 0.7618, NDCG = 0.5387\n",
      "21:34:38 - Epoch 082: Loss = 0.1807\n",
      "21:34:43 - Epoch 082: Loss = 0.1903 HR = 0.7604, NDCG = 0.5358\n",
      "21:35:11 - Epoch 083: Loss = 0.1792\n",
      "21:35:17 - Epoch 083: Loss = 0.1738 HR = 0.7611, NDCG = 0.5346\n",
      "21:35:45 - Epoch 084: Loss = 0.1784\n",
      "21:35:50 - Epoch 084: Loss = 0.1813 HR = 0.7569, NDCG = 0.5352\n",
      "21:36:19 - Epoch 085: Loss = 0.1766\n",
      "21:36:24 - Epoch 085: Loss = 0.1843 HR = 0.7607, NDCG = 0.5386\n",
      "21:36:52 - Epoch 086: Loss = 0.1755\n",
      "21:36:58 - Epoch 086: Loss = 0.1776 HR = 0.7616, NDCG = 0.5391\n",
      "21:37:26 - Epoch 087: Loss = 0.1744\n",
      "21:37:31 - Epoch 087: Loss = 0.1794 HR = 0.7575, NDCG = 0.5336\n",
      "21:37:59 - Epoch 088: Loss = 0.1744\n",
      "21:38:05 - Epoch 088: Loss = 0.1772 HR = 0.7571, NDCG = 0.5366\n",
      "21:38:33 - Epoch 089: Loss = 0.1735\n",
      "21:38:38 - Epoch 089: Loss = 0.1758 HR = 0.7572, NDCG = 0.5370\n",
      "21:39:06 - Epoch 090: Loss = 0.1713\n",
      "21:39:12 - Epoch 090: Loss = 0.1696 HR = 0.7578, NDCG = 0.5345\n",
      "21:39:40 - Epoch 091: Loss = 0.1720\n",
      "21:39:45 - Epoch 091: Loss = 0.1731 HR = 0.7600, NDCG = 0.5384\n",
      "21:40:14 - Epoch 092: Loss = 0.1708\n",
      "21:40:19 - Epoch 092: Loss = 0.1611 HR = 0.7581, NDCG = 0.5328\n",
      "21:40:47 - Epoch 093: Loss = 0.1708\n",
      "21:40:53 - Epoch 093: Loss = 0.1570 HR = 0.7520, NDCG = 0.5311\n",
      "21:41:21 - Epoch 094: Loss = 0.1689\n",
      "21:41:26 - Epoch 094: Loss = 0.1676 HR = 0.7528, NDCG = 0.5258\n",
      "21:41:54 - Epoch 095: Loss = 0.1688\n",
      "21:42:00 - Epoch 095: Loss = 0.1713 HR = 0.7602, NDCG = 0.5321\n",
      "21:42:28 - Epoch 096: Loss = 0.1674\n",
      "21:42:34 - Epoch 096: Loss = 0.1703 HR = 0.7595, NDCG = 0.5384\n",
      "21:43:02 - Epoch 097: Loss = 0.1673\n",
      "21:43:07 - Epoch 097: Loss = 0.1719 HR = 0.7608, NDCG = 0.5385\n",
      "21:43:35 - Epoch 098: Loss = 0.1657\n",
      "21:43:41 - Epoch 098: Loss = 0.1672 HR = 0.7581, NDCG = 0.5368\n",
      "21:44:09 - Epoch 099: Loss = 0.1657\n",
      "21:44:14 - Epoch 099: Loss = 0.1659 HR = 0.7577, NDCG = 0.5365\n",
      "21:44:43 - Epoch 100: Loss = 0.1646\n",
      "21:44:48 - Epoch 100: Loss = 0.1615 HR = 0.7595, NDCG = 0.5383\n",
      "21:45:16 - Epoch 101: Loss = 0.1642\n",
      "21:45:22 - Epoch 101: Loss = 0.1621 HR = 0.7584, NDCG = 0.5349\n",
      "21:45:50 - Epoch 102: Loss = 0.1632\n",
      "21:45:55 - Epoch 102: Loss = 0.1695 HR = 0.7617, NDCG = 0.5402\n",
      "21:46:24 - Epoch 103: Loss = 0.1625\n",
      "21:46:29 - Epoch 103: Loss = 0.1594 HR = 0.7584, NDCG = 0.5369\n",
      "21:46:57 - Epoch 104: Loss = 0.1629\n",
      "21:47:03 - Epoch 104: Loss = 0.1632 HR = 0.7558, NDCG = 0.5337\n",
      "21:47:31 - Epoch 105: Loss = 0.1614\n",
      "21:47:37 - Epoch 105: Loss = 0.1598 HR = 0.7528, NDCG = 0.5303\n",
      "21:48:05 - Epoch 106: Loss = 0.1610\n",
      "21:48:10 - Epoch 106: Loss = 0.1715 HR = 0.7600, NDCG = 0.5364\n",
      "21:48:38 - Epoch 107: Loss = 0.1606\n",
      "21:48:44 - Epoch 107: Loss = 0.1570 HR = 0.7584, NDCG = 0.5349\n",
      "21:49:12 - Epoch 108: Loss = 0.1590\n",
      "21:49:17 - Epoch 108: Loss = 0.1738 HR = 0.7584, NDCG = 0.5360\n",
      "21:49:46 - Epoch 109: Loss = 0.1577\n",
      "21:49:51 - Epoch 109: Loss = 0.1510 HR = 0.7575, NDCG = 0.5360\n",
      "21:50:19 - Epoch 110: Loss = 0.1585\n",
      "21:50:25 - Epoch 110: Loss = 0.1525 HR = 0.7561, NDCG = 0.5373\n",
      "21:50:53 - Epoch 111: Loss = 0.1573\n",
      "21:50:58 - Epoch 111: Loss = 0.1706 HR = 0.7544, NDCG = 0.5377\n",
      "21:51:27 - Epoch 112: Loss = 0.1583\n",
      "21:51:32 - Epoch 112: Loss = 0.1665 HR = 0.7600, NDCG = 0.5392\n",
      "21:52:00 - Epoch 113: Loss = 0.1570\n",
      "21:52:06 - Epoch 113: Loss = 0.1542 HR = 0.7500, NDCG = 0.5325\n",
      "21:52:34 - Epoch 114: Loss = 0.1568\n",
      "21:52:39 - Epoch 114: Loss = 0.1574 HR = 0.7567, NDCG = 0.5360\n",
      "21:53:07 - Epoch 115: Loss = 0.1553\n",
      "21:53:13 - Epoch 115: Loss = 0.1523 HR = 0.7608, NDCG = 0.5370\n",
      "21:53:41 - Epoch 116: Loss = 0.1552\n",
      "21:53:46 - Epoch 116: Loss = 0.1473 HR = 0.7586, NDCG = 0.5342\n",
      "21:54:15 - Epoch 117: Loss = 0.1542\n",
      "21:54:20 - Epoch 117: Loss = 0.1497 HR = 0.7545, NDCG = 0.5344\n",
      "21:54:48 - Epoch 118: Loss = 0.1540\n",
      "21:54:54 - Epoch 118: Loss = 0.1534 HR = 0.7598, NDCG = 0.5400\n",
      "21:55:22 - Epoch 119: Loss = 0.1535\n",
      "21:55:28 - Epoch 119: Loss = 0.1541 HR = 0.7518, NDCG = 0.5308\n",
      "21:55:56 - Epoch 120: Loss = 0.1533\n",
      "21:56:02 - Epoch 120: Loss = 0.1569 HR = 0.7555, NDCG = 0.5392\n",
      "21:56:30 - Epoch 121: Loss = 0.1521\n",
      "21:56:35 - Epoch 121: Loss = 0.1531 HR = 0.7575, NDCG = 0.5376\n",
      "21:57:03 - Epoch 122: Loss = 0.1526\n",
      "21:57:09 - Epoch 122: Loss = 0.1565 HR = 0.7636, NDCG = 0.5410\n",
      "21:57:37 - Epoch 123: Loss = 0.1512\n",
      "21:57:43 - Epoch 123: Loss = 0.1476 HR = 0.7581, NDCG = 0.5374\n",
      "21:58:11 - Epoch 124: Loss = 0.1513\n",
      "21:58:16 - Epoch 124: Loss = 0.1484 HR = 0.7604, NDCG = 0.5349\n",
      "21:58:44 - Epoch 125: Loss = 0.1509\n",
      "21:58:50 - Epoch 125: Loss = 0.1469 HR = 0.7559, NDCG = 0.5334\n",
      "21:59:18 - Epoch 126: Loss = 0.1494\n",
      "21:59:23 - Epoch 126: Loss = 0.1454 HR = 0.7579, NDCG = 0.5380\n",
      "21:59:52 - Epoch 127: Loss = 0.1514\n",
      "21:59:57 - Epoch 127: Loss = 0.1486 HR = 0.7603, NDCG = 0.5389\n",
      "22:00:25 - Epoch 128: Loss = 0.1500\n",
      "22:00:31 - Epoch 128: Loss = 0.1438 HR = 0.7573, NDCG = 0.5380\n",
      "22:00:59 - Epoch 129: Loss = 0.1486\n",
      "22:01:04 - Epoch 129: Loss = 0.1535 HR = 0.7573, NDCG = 0.5383\n",
      "22:01:33 - Epoch 130: Loss = 0.1485\n",
      "22:01:38 - Epoch 130: Loss = 0.1372 HR = 0.7571, NDCG = 0.5358\n",
      "22:02:06 - Epoch 131: Loss = 0.1482\n",
      "22:02:12 - Epoch 131: Loss = 0.1536 HR = 0.7607, NDCG = 0.5386\n",
      "22:02:40 - Epoch 132: Loss = 0.1476\n",
      "22:02:45 - Epoch 132: Loss = 0.1711 HR = 0.7593, NDCG = 0.5416\n",
      "22:03:13 - Epoch 133: Loss = 0.1462\n",
      "22:03:19 - Epoch 133: Loss = 0.1454 HR = 0.7590, NDCG = 0.5379\n",
      "22:03:47 - Epoch 134: Loss = 0.1464\n",
      "22:03:52 - Epoch 134: Loss = 0.1417 HR = 0.7567, NDCG = 0.5391\n",
      "22:04:20 - Epoch 135: Loss = 0.1478\n",
      "22:04:26 - Epoch 135: Loss = 0.1512 HR = 0.7592, NDCG = 0.5366\n",
      "22:04:54 - Epoch 136: Loss = 0.1474\n",
      "22:04:59 - Epoch 136: Loss = 0.1428 HR = 0.7588, NDCG = 0.5368\n",
      "22:05:27 - Epoch 137: Loss = 0.1452\n",
      "22:05:33 - Epoch 137: Loss = 0.1504 HR = 0.7598, NDCG = 0.5403\n",
      "22:06:01 - Epoch 138: Loss = 0.1458\n",
      "22:06:07 - Epoch 138: Loss = 0.1387 HR = 0.7638, NDCG = 0.5412\n",
      "22:06:35 - Epoch 139: Loss = 0.1449\n",
      "22:06:40 - Epoch 139: Loss = 0.1540 HR = 0.7601, NDCG = 0.5399\n",
      "22:07:08 - Epoch 140: Loss = 0.1446\n",
      "22:07:14 - Epoch 140: Loss = 0.1400 HR = 0.7560, NDCG = 0.5348\n",
      "22:07:42 - Epoch 141: Loss = 0.1445\n",
      "22:07:47 - Epoch 141: Loss = 0.1505 HR = 0.7594, NDCG = 0.5349\n",
      "22:08:15 - Epoch 142: Loss = 0.1435\n",
      "22:08:21 - Epoch 142: Loss = 0.1464 HR = 0.7576, NDCG = 0.5376\n",
      "22:08:49 - Epoch 143: Loss = 0.1420\n",
      "22:08:55 - Epoch 143: Loss = 0.1426 HR = 0.7591, NDCG = 0.5398\n",
      "22:09:23 - Epoch 144: Loss = 0.1430\n",
      "22:09:28 - Epoch 144: Loss = 0.1378 HR = 0.7566, NDCG = 0.5376\n",
      "22:09:56 - Epoch 145: Loss = 0.1422\n",
      "22:10:02 - Epoch 145: Loss = 0.1387 HR = 0.7577, NDCG = 0.5345\n",
      "22:10:30 - Epoch 146: Loss = 0.1420\n",
      "22:10:36 - Epoch 146: Loss = 0.1409 HR = 0.7578, NDCG = 0.5371\n",
      "22:11:04 - Epoch 147: Loss = 0.1416\n",
      "22:11:09 - Epoch 147: Loss = 0.1465 HR = 0.7569, NDCG = 0.5375\n",
      "22:11:37 - Epoch 148: Loss = 0.1410\n",
      "22:11:43 - Epoch 148: Loss = 0.1397 HR = 0.7566, NDCG = 0.5378\n",
      "22:12:11 - Epoch 149: Loss = 0.1408\n",
      "22:12:16 - Epoch 149: Loss = 0.1468 HR = 0.7604, NDCG = 0.5382\n",
      "22:12:44 - Epoch 150: Loss = 0.1408\n",
      "22:12:50 - Epoch 150: Loss = 0.1506 HR = 0.7615, NDCG = 0.5381\n",
      "22:13:18 - Epoch 151: Loss = 0.1393\n",
      "22:13:23 - Epoch 151: Loss = 0.1542 HR = 0.7567, NDCG = 0.5379\n",
      "22:13:51 - Epoch 152: Loss = 0.1405\n",
      "22:13:57 - Epoch 152: Loss = 0.1435 HR = 0.7606, NDCG = 0.5387\n",
      "22:14:25 - Epoch 153: Loss = 0.1398\n",
      "22:14:31 - Epoch 153: Loss = 0.1454 HR = 0.7570, NDCG = 0.5352\n",
      "22:14:59 - Epoch 154: Loss = 0.1374\n",
      "22:15:04 - Epoch 154: Loss = 0.1437 HR = 0.7525, NDCG = 0.5327\n",
      "22:15:32 - Epoch 155: Loss = 0.1389\n",
      "22:15:38 - Epoch 155: Loss = 0.1510 HR = 0.7585, NDCG = 0.5353\n",
      "22:16:06 - Epoch 156: Loss = 0.1391\n",
      "22:16:11 - Epoch 156: Loss = 0.1452 HR = 0.7621, NDCG = 0.5409\n",
      "22:16:39 - Epoch 157: Loss = 0.1376\n",
      "22:16:45 - Epoch 157: Loss = 0.1437 HR = 0.7578, NDCG = 0.5390\n",
      "22:17:13 - Epoch 158: Loss = 0.1382\n",
      "22:17:19 - Epoch 158: Loss = 0.1299 HR = 0.7613, NDCG = 0.5408\n",
      "22:17:47 - Epoch 159: Loss = 0.1373\n",
      "22:17:52 - Epoch 159: Loss = 0.1509 HR = 0.7605, NDCG = 0.5380\n",
      "22:18:20 - Epoch 160: Loss = 0.1366\n",
      "22:18:26 - Epoch 160: Loss = 0.1355 HR = 0.7602, NDCG = 0.5394\n",
      "22:18:54 - Epoch 161: Loss = 0.1370\n",
      "22:18:59 - Epoch 161: Loss = 0.1470 HR = 0.7635, NDCG = 0.5406\n",
      "22:19:27 - Epoch 162: Loss = 0.1369\n",
      "22:19:33 - Epoch 162: Loss = 0.1414 HR = 0.7605, NDCG = 0.5388\n",
      "22:20:01 - Epoch 163: Loss = 0.1371\n",
      "22:20:07 - Epoch 163: Loss = 0.1363 HR = 0.7601, NDCG = 0.5362\n",
      "22:20:35 - Epoch 164: Loss = 0.1356\n",
      "22:20:40 - Epoch 164: Loss = 0.1460 HR = 0.7606, NDCG = 0.5409\n",
      "22:21:08 - Epoch 165: Loss = 0.1360\n",
      "22:21:14 - Epoch 165: Loss = 0.1370 HR = 0.7592, NDCG = 0.5382\n",
      "22:21:42 - Epoch 166: Loss = 0.1357\n",
      "22:21:47 - Epoch 166: Loss = 0.1437 HR = 0.7581, NDCG = 0.5351\n",
      "22:22:15 - Epoch 167: Loss = 0.1356\n",
      "22:22:21 - Epoch 167: Loss = 0.1306 HR = 0.7570, NDCG = 0.5381\n",
      "22:22:49 - Epoch 168: Loss = 0.1340\n",
      "22:22:55 - Epoch 168: Loss = 0.1344 HR = 0.7568, NDCG = 0.5396\n",
      "22:23:23 - Epoch 169: Loss = 0.1341\n",
      "22:23:28 - Epoch 169: Loss = 0.1449 HR = 0.7602, NDCG = 0.5415\n",
      "22:23:56 - Epoch 170: Loss = 0.1353\n",
      "22:24:02 - Epoch 170: Loss = 0.1402 HR = 0.7556, NDCG = 0.5384\n",
      "22:24:30 - Epoch 171: Loss = 0.1339\n",
      "22:24:35 - Epoch 171: Loss = 0.1422 HR = 0.7580, NDCG = 0.5420\n",
      "22:25:03 - Epoch 172: Loss = 0.1342\n",
      "22:25:09 - Epoch 172: Loss = 0.1383 HR = 0.7591, NDCG = 0.5402\n",
      "22:25:37 - Epoch 173: Loss = 0.1337\n",
      "22:25:43 - Epoch 173: Loss = 0.1438 HR = 0.7593, NDCG = 0.5356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train(\n\u001b[0;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      3\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[0;32m      4\u001b[0m     val_loader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[0;32m      5\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m      6\u001b[0m     optim\u001b[39m=\u001b[39;49moptim,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m      8\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m      9\u001b[0m     checkpoint\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./\u001b[39;49m\u001b[39m{\u001b[39;49;00mexp_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     10\u001b[0m     \u001b[39m# scheduler=scheduler\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m )\n",
      "File \u001b[1;32mc:\\Data\\Kinit\\replication_study\\carca-replication\\notebooks\\..\\carca\\train.py:89\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, device, optim, epochs, top_k, verbose, early_stop, checkpoint, scheduler)\u001b[0m\n\u001b[0;32m     86\u001b[0m loss_mask \u001b[39m=\u001b[39m get_mask(o_x)\n\u001b[0;32m     87\u001b[0m loss \u001b[39m=\u001b[39m loss_fn\u001b[39m.\u001b[39mforward(y_pred, y_true, loss_mask)\n\u001b[1;32m---> 89\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     90\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     91\u001b[0m sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\42190\\.conda\\envs\\carca\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\42190\\.conda\\envs\\carca\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    optim=optim,\n",
    "    epochs=epochs,\n",
    "    early_stop=50,\n",
    "    checkpoint=f\"./{exp_name}\"\n",
    "    # scheduler=scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c439b96c6fb0888fbb7bef077785ff873b0265e2f630b6096d8ea5b47f36cd00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
